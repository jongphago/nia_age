{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from main_ae import AgeModel\n",
    "from main_ae import START_AGE, END_AGE, NUM_AGE_GROUPS, LAMBDA_1, LAMBDA_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AGE_GROUPS = 9\n",
    "VALIDATION_RATE = 0.1\n",
    "num_ages = END_AGE - START_AGE + 1\n",
    "\n",
    "random.seed(2019)\n",
    "np.random.seed(2019)\n",
    "torch.manual_seed(2019)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIHub dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpt.data import join_face_df\n",
    "from fpt.path import DTFR\n",
    "\n",
    "DATA_CATEGORY = \"aihub_family\"\n",
    "face_df = join_face_df(DTFR, DATA_CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANGE_TO_MEDIAN = {\n",
    "    \"a\": (1 + 6) / 2,\n",
    "    \"b\": (7 + 12) / 2,\n",
    "    \"c\": (13 + 19) / 2,\n",
    "    \"d\": (20 + 30) / 2,\n",
    "    \"e\": (31 + 45) / 2,\n",
    "    \"f\": (46 + 55) / 2,\n",
    "    \"g\": (56 + 66) / 2,\n",
    "    \"h\": (67 + 80) / 2,\n",
    "    \"above\": 90,\n",
    "}\n",
    "AGE_GROUPS = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"above\"]\n",
    "\n",
    "\n",
    "def age_to_age_groups(age):\n",
    "    if age <= 6:\n",
    "        return \"a\"\n",
    "    if age <= 12:\n",
    "        return \"b\"\n",
    "    if age <= 19:\n",
    "        return \"c\"\n",
    "    if age <= 30:\n",
    "        return \"d\"\n",
    "    if age <= 45:\n",
    "        return \"e\"\n",
    "    if age <= 55:\n",
    "        return \"f\"\n",
    "    if age <= 66:\n",
    "        return \"g\"\n",
    "    if age <= 80:\n",
    "        return \"h\"\n",
    "    return \"above\"\n",
    "\n",
    "GROUP_TO_INDEX = {group: index for index, group in enumerate(AGE_GROUPS)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIA Age Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgeModel(num_ages, NUM_AGE_GROUPS)  # age_pred, age_group_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import NiaDataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_path = \"nia_cropped/train_0.npy\"\n",
    "test_meta_path = \"nia_cropped/test_0.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NiaDataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.RandomApply(\n",
    "            [\n",
    "                torchvision.transforms.RandomAffine(degrees=10, shear=16),\n",
    "                torchvision.transforms.RandomHorizontalFlip(p=1.0),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        torchvision.transforms.Resize((256, 256)),\n",
    "        torchvision.transforms.RandomCrop((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = NiaDataset(train_meta_path, transforms_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_gen,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(train_iter)\n",
    "sample.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sample.keys():\n",
    "    print(f\"{key}:\\t{sample[key][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_df.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "file_name, _ = os.path.splitext(sample['file'][idx])\n",
    "file_name in face_df.key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "val_gen = NiaDataset(test_meta_path, transforms)\n",
    "val_loader = DataLoader(\n",
    "    val_gen, batch_size=1, shuffle=False, pin_memory=True, num_workers=0\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA_1 = 0.2\n",
    "LAMBDA_2 = 0.05\n",
    "START_AGE = 0\n",
    "END_AGE = 90\n",
    "learning_rate = 1e-3\n",
    "epoch = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[5, 8, 9], gamma=0.1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mean_variance_loss import MeanVarianceLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = MeanVarianceLoss(LAMBDA_1, LAMBDA_2, START_AGE, END_AGE).cuda\n",
    "criterion2 = torch.nn.CrossEntropyLoss().cuda()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_ae import train_softmax, evaluate_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_softmax(train_loader, model, criterion2, optimizer, epoch, result_directory):\n",
    "    model.cuda().train()\n",
    "    running_loss = 0.0\n",
    "    running_softmax_loss = 0.0\n",
    "    interval = 1\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        images = sample[\"image\"].cuda()\n",
    "        labels = sample[\"age_class\"].cuda()\n",
    "        _, output = model(images)\n",
    "        loss = criterion2(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data\n",
    "        if (i + 1) % interval == 0:\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch, i, running_loss / interval))\n",
    "            with open(os.path.join(result_directory, \"log\"), \"a\") as f:\n",
    "                f.write(\"[%d, %5d] loss: %.3f\\n\" % (epoch, i, running_loss / interval))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    break\n",
    "    train_softmax(train_loader, model, criterion2, optimizer, epoch, \"result\")\n",
    "    loss_val, mae = evaluate_softmax(val_loader, model, criterion2)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_val, mae\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcface_torch.losses import CombinedMarginLoss\n",
    "from arcface_torch.configs.aihub_r50_onegpu import config as aihub_config\n",
    "from arcface_torch.configs.base import config as cfg\n",
    "\n",
    "cfg.update(aihub_config)\n",
    "cfg.output = \"work_dirs/aihub_r50_onegpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_loss = CombinedMarginLoss(\n",
    "    64,\n",
    "    cfg.margin_list[0],\n",
    "    cfg.margin_list[1],\n",
    "    cfg.margin_list[2],\n",
    "    cfg.interclass_filtering_threshold,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHubDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet.datasets.AIHubDataset import AIHubDataset\n",
    "# from to\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/jupyter/data/face-image/train_aihub_family'\n",
    "image_size = 112\n",
    "aihub_mean = [0.5444, 0.4335, 0.3800]\n",
    "aihub_std = [0.2672, 0.2295, 0.2156]\n",
    "face_age_transform_train = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(size=(image_size, image_size)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=aihub_mean, std=aihub_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ImageFolder(root_dir, face_age_transform_train)\n",
    "next(iter(train_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi task Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from easydict import EasyDict as edict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/jupyter/data/face-image/train_aihub_family'\n",
    "image_size = 112\n",
    "aihub_mean = [0.5444, 0.4335, 0.3800]\n",
    "aihub_std = [0.2672, 0.2295, 0.2156]\n",
    "face_age_transform_train = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(size=(image_size, image_size)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=aihub_mean, std=aihub_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_age_transforms_train_nia = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.RandomApply(\n",
    "            [\n",
    "                torchvision.transforms.RandomAffine(degrees=10, shear=16),\n",
    "                torchvision.transforms.RandomHorizontalFlip(p=1.0),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        torchvision.transforms.Resize((256, 256)),\n",
    "        torchvision.transforms.RandomCrop((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceAgeDataset(Dataset):\n",
    "    def __init__(self, root_dir, face_df, transform):\n",
    "        self.face_dataset = ImageFolder(root=root_dir, transform=transform)\n",
    "        self.face_df = face_df\n",
    "        self.class_to_idx = self.face_dataset.class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.face_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, face_label = self.face_dataset[index]\n",
    "        path, _ = self.face_dataset.samples[index]\n",
    "        *_, key = os.path.splitext(path)[0].split(\"/\")\n",
    "        row = face_df.loc[key]\n",
    "        sample = edict({\n",
    "            \"image\": image,\n",
    "            \"age\": row.age,\n",
    "            \"age_class\": GROUP_TO_INDEX[row.age_group],\n",
    "            \"file\": path,\n",
    "            \"data_type\": row.category,\n",
    "            \"family_id\": row.family_id,\n",
    "            \"personal_id\": row.target,\n",
    "            \"face_label\": face_label,\n",
    "            \"key\": key,\n",
    "        })\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/jupyter/data/face-image/train_aihub_family\"\n",
    "face_age_dataset = FaceAgeDataset(\n",
    "    root_dir,\n",
    "    face_df,\n",
    "    face_age_transform_train,\n",
    ")\n",
    "iterator = iter(face_age_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iterator)\n",
    "[print(f\"{k}: {v}\") for k, v in sample.items()]\n",
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet.datasets.AIHubDataset import AIHubDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_root_dir = \"/home/jupyter/data/face-image/valid_aihub_family\"\n",
    "num_workers = 4\n",
    "lfw_batch_size = 200\n",
    "image_size = 112\n",
    "aihub_mean = [0.5444, 0.4335, 0.3800]\n",
    "aihub_std = [0.2672, 0.2295, 0.2156]\n",
    "\n",
    "face_age_transform_valid = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=aihub_mean, std=aihub_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_age_valid_dataset = FaceAgeDataset(\n",
    "    valid_root_dir,\n",
    "    face_df,\n",
    "    face_age_transform_valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iter = iter(face_age_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sample = next(valid_iter)\n",
    "valid_sample.family_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_age_verification_valid_dataset = AIHubDataset(\n",
    "    dir=valid_root_dir,\n",
    "    pairs_path=\"data/pairs/valid/pairs_Age.txt\",\n",
    "    transform=face_age_transform_valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_dataset = iter(face_age_verification_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_valid_sample = next(pairs_dataset)  # img1, img2, issame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    data = {\n",
    "        key: default_collate([d[key] for d in batch])\n",
    "        for key in batch[0]\n",
    "        if key != \"age\"\n",
    "    }\n",
    "    age = torch.tensor(\n",
    "        [d[\"age\"] for d in batch], dtype=torch.float32\n",
    "    )  # 'age' 필드를 텐서로 변환합니다.\n",
    "    data[\"age\"] = age\n",
    "    return data\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "face_age_loader = DataLoader(\n",
    "    dataset=face_age_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_iter = iter(face_age_loader)\n",
    "sample = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.family_id)\n",
    "print(sample.age_class)\n",
    "print(sample.personal_id)\n",
    "print(sample.face_label)\n",
    "sample.image.shape, sample.age"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "face_age_loader = DataLoader(\n",
    "    dataset=face_age_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_iter = iter(face_age_loader)\n",
    "sample = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_age_valid_loader = DataLoader(\n",
    "    face_age_valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aihub_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=face_age_verification_valid_dataset,\n",
    "    batch_size=lfw_batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_iter = iter(aihub_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(verification_iter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nia_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_ae import AgeModel, Embedding\n",
    "from main_ae import END_AGE, START_AGE, NUM_AGE_GROUPS\n",
    "NUM_AGES = END_AGE - START_AGE + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgeModel(NUM_AGES, NUM_AGE_GROUPS)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sample.image.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_pred, age_group_pred = model(images)\n",
    "age_pred.shape, age_group_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding()\n",
    "embedding = embedding.cuda()\n",
    "embedding = embedding.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nia_embedding_result = embedding(images)\n",
    "nia_embedding_result.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArcFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcface_torch.backbones import get_model\n",
    "from arcface_torch.configs.aihub_r50_onegpu import config as aihub_config\n",
    "from arcface_torch.configs.base import config as cfg\n",
    "\n",
    "cfg.update(aihub_config)\n",
    "cfg.output = \"work_dirs/aihub_r50_onegpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = get_model(\n",
    "    cfg.network,\n",
    "    dropout=0.0,\n",
    "    fp16=False,\n",
    "    num_features=512,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = torch.load(\n",
    "    f\"/home/jupyter/family-photo-tree/utils/model/arcface/{cfg.network}/backbone.pth\"\n",
    ")\n",
    "backbone.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = backbone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcface_result = backbone(images)\n",
    "arcface_result.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age Classifier with ArcFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_ae import AgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_classifier = AgeClassifier(NUM_AGES, NUM_AGE_GROUPS).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_age_with_arcface, pred_age_group_with_arcface = age_classifier(arcface_result)\n",
    "pred_age_with_arcface.shape, pred_age_group_with_arcface.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss_nia = torch.nn.CrossEntropyLoss().cuda()\n",
    "age_loss = cross_entropy_loss_nia(age_pred, sample.age.cuda())\n",
    "age_group_loss = cross_entropy_loss_nia(age_group_pred, sample.age_class.cuda()) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-Variance Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mean_variance_loss import MeanVarianceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_variance_loss = MeanVarianceLoss(LAMBDA_1, LAMBDA_2, START_AGE, END_AGE).cuda()\n",
    "mean_loss, variance_loss = mean_variance_loss(age_pred, sample.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_nia = mean_loss + variance_loss + age_loss + 10 * age_group_loss\n",
    "total_loss_nia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CombinedMarginLoss | ArcFace | CosFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcface_torch.losses import CombinedMarginLoss, ArcFace, CosFace\n",
    "from torch.nn.functional import normalize, linear\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "num_classes = len(face_age_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_loss = CombinedMarginLoss(\n",
    "    64,\n",
    "    cfg.margin_list[0],\n",
    "    cfg.margin_list[1],\n",
    "    cfg.margin_list[2],\n",
    "    cfg.interclass_filtering_threshold,\n",
    ")\n",
    "arc_face_loss = ArcFace()\n",
    "cos_face_loss = CosFace()\n",
    "cross_entropy_loss_arcface = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "labels = sample.face_label.cuda()\n",
    "labels.squeeze_()\n",
    "labels = labels.long()\n",
    "labels = labels.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "norm_embeddings = normalize(arcface_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight\n",
    "weight = torch.nn.Parameter(torch.normal(0, 0.01, (num_classes, 512))).cuda()\n",
    "norm_weight_activated = normalize(weight)\n",
    "norm_weight_activated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits\n",
    "logits = linear(norm_embeddings, norm_weight_activated)\n",
    "logits = logits.clamp(-1, 1)\n",
    "# softmax = margin_loss(logits, labels)\n",
    "# softmax = arc_face_loss(logits, labels)\n",
    "softmax = cos_face_loss(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss_arcface = cross_entropy_loss_arcface(softmax, labels.flatten())\n",
    "loss_arcface"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from arcface_torch.lr_scheduler import PolyScheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nia_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_nia = Adam(model.parameters(), lr=0.001)\n",
    "scheduler_nia = StepLR(optimizer_nia, 5)\n",
    "optimizer_nia.zero_grad()\n",
    "total_loss_nia.backward()\n",
    "optimizer_nia.step()\n",
    "scheduler_nia.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArcFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = 0.9  #\n",
    "weight_decay = 5e-4  #\n",
    "lr = 0.02\n",
    "\n",
    "opt = torch.optim.SGD(\n",
    "    params=[\n",
    "        {\"params\": backbone.parameters()},\n",
    "        {\"params\": margin_loss.parameters()},\n",
    "    ],\n",
    "    lr=lr,\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 2\n",
    "world_size = 1\n",
    "cfg.total_batch_size = cfg.batch_size * world_size\n",
    "cfg.warmup_step = cfg.num_image // cfg.total_batch_size * cfg.warmup_epoch\n",
    "cfg.total_step = cfg.num_image // cfg.total_batch_size * num_epoch\n",
    "\n",
    "lr_scheduler = PolyScheduler(\n",
    "    optimizer=opt,\n",
    "    base_lr=lr,\n",
    "    max_steps=cfg.total_step,\n",
    "    warmup_steps=cfg.warmup_step,\n",
    "    last_epoch=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()\n",
    "loss_arcface.backward()\n",
    "opt.step()\n",
    "lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# 시각화\n",
    "dot = make_dot(total_loss_nia, params=dict(model.named_parameters()))\n",
    "dot.format = \"pdf\"  # 'png'로 형식 변경\n",
    "dot.render(\"graph\")  # 'graph.png' 파일로 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NIA Age\n",
    "#### train softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.\n",
    "interval = 10\n",
    "for index, sample in enumerate(face_age_loader):\n",
    "    images = sample.image.cuda()\n",
    "    labels = sample.age.cuda()\n",
    "    age_pred, age_group_red = model(images)\n",
    "    age_loss = cross_entropy_loss_nia(age_pred, labels)\n",
    "    optimizer_nia.zero_grad()\n",
    "    age_loss.backward()\n",
    "    optimizer_nia.step()\n",
    "    running_loss += age_loss.data\n",
    "    if (index + 1) % interval == 0:\n",
    "        print(f\"loss: {running_loss/interval:.3f}\")\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age & Age_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "interval = 10\n",
    "\n",
    "for index, sample in enumerate(face_age_loader):\n",
    "    images = sample.image.cuda()\n",
    "    labels = sample.age.cuda()\n",
    "    age_group_labels = sample.age_class.cuda()\n",
    "    age_pred, age_group_pred = model(images)\n",
    "    \n",
    "    dta = np.array(sample.data_type)\n",
    "    age_sample_indices = dta != \"Age\"\n",
    "    age_pred = age_pred[age_sample_indices]\n",
    "    labels = labels[age_sample_indices]\n",
    "    \n",
    "    mean_loss, variance_loss = mean_variance_loss(age_pred, labels)\n",
    "    age_softmax_loss = cross_entropy_loss_nia(age_pred, labels)\n",
    "    mean_loss, variance_loss, age_softmax_loss\n",
    "    \n",
    "    age_group_pred = age_group_pred[~age_sample_indices]\n",
    "    age_group_labels = age_group_labels[~age_sample_indices]\n",
    "    age_group_softmax_loss = cross_entropy_loss_nia(age_group_pred, age_group_labels)\n",
    "    \n",
    "    total_loss = mean_loss + variance_loss + age_softmax_loss + 10 * age_group_softmax_loss\n",
    "    \n",
    "    optimizer_nia.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer_nia.step()\n",
    "    \n",
    "    running_loss += total_loss.data\n",
    "    if (index + 1) % interval == 0:\n",
    "        print(f\"loss: {running_loss/interval:.3f}\")\n",
    "        running_loss = 0.0\n",
    "    # break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import NiaDataset\n",
    "from main_ae import evaluate_1, AgeModel\n",
    "from main_ae import START_AGE, END_AGE, NUM_AGE_GROUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = \"/home/jongphago/nia_age/result_model/model_0\"\n",
    "test_meta_path = \"nia_cropped/test_0.npy\"\n",
    "val_gen = NiaDataset(test_meta_path, transforms)\n",
    "val_loader = DataLoader(\n",
    "    val_gen,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgeModel(END_AGE - START_AGE + 1, NUM_AGE_GROUPS)\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load(save_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nia_valid_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "val_gen = NiaDataset(test_meta_path, nia_valid_transforms)\n",
    "val_loader = DataLoader(\n",
    "    val_gen,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, ag_acc, log_dict = evaluate_1(val_loader, model)  # ag_acc 0.6746\n",
    "print(\"Mae---\", mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `evaluate_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = torch.load(save_model_path)\n",
    "model.load_state_dict(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def display_sample(sample):\n",
    "    # DataLoader에서 첫 번째 이미지를 가져옵니다.\n",
    "    image_tensor = sample[\"image\"].cpu().numpy()\n",
    "\n",
    "    # 이미지 텐서의 차원을 줄입니다.\n",
    "    image_array = np.squeeze(image_tensor)\n",
    "\n",
    "    # 배열의 축을 재배치합니다 (채널을 마지막 축으로 이동).\n",
    "    image_array = image_array.transpose(1, 2, 0)\n",
    "\n",
    "    # [0, 1] 범위에서 [0, 255] 범위로 스케일링합니다.\n",
    "    image_array = (image_array * 255).astype(np.uint8)\n",
    "\n",
    "    # 이미지 배열을 PIL 이미지로 변환합니다.\n",
    "    image = Image.fromarray(image_array)\n",
    "\n",
    "    # 이미지 크기를 조절합니다.\n",
    "    resized_image = image.resize((80, 80))\n",
    "\n",
    "    # 이미지를 표시합니다.\n",
    "    display(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_evaluate(val_loader, model, task=(\"kinship\")):\n",
    "    log_dict = dict()\n",
    "    log_dict[\"No.\"] = []\n",
    "    log_dict[\"image_path\"] = []\n",
    "    log_dict[\"data_type\"] = []\n",
    "    log_dict[\"GT_age\"] = []\n",
    "    log_dict[\"predicted_age\"] = []\n",
    "    log_dict[\"GT_age_group\"] = []\n",
    "    log_dict[\"predicted_age_group\"] = []\n",
    "    log_dict[\"absolute_error\"] = []\n",
    "    log_dict[\"mean_absolute_error\"] = []\n",
    "    log_dict[\"age_group_classification_accuracy\"] = []\n",
    "\n",
    "    model.eval()\n",
    "    ae_sum = 0.0\n",
    "    kinship_counter = 0\n",
    "    age_counter = 0\n",
    "    age_group_success = 0\n",
    "    tics = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, sample in tqdm(enumerate(val_loader)):\n",
    "            image = sample[\"image\"].cuda()\n",
    "            output, output2 = model(image)\n",
    "            m = torch.nn.Softmax(dim=1)\n",
    "            output_softmax = m(output)\n",
    "\n",
    "            a = torch.arange(START_AGE, END_AGE + 1, dtype=torch.float32).cuda()\n",
    "            mean = (output_softmax * a).sum(1, keepdim=True).cpu().data.numpy()\n",
    "            pred = np.around(mean)[0][0]\n",
    "            pred_age_group = np.argmax(output2.cpu().data.numpy())\n",
    "            gt_age_group = sample[\"age_class\"].cpu().item()\n",
    "            gt_age = sample[\"age\"].cpu().item()\n",
    "            ae = np.absolute(pred - gt_age)\n",
    "\n",
    "            filename = sample[\"file\"][0]\n",
    "            log_dict[\"No.\"].append(i)\n",
    "            log_dict[\"image_path\"].append(filename)\n",
    "            data_type = sample[\"data_type\"][0]\n",
    "            log_dict[\"data_type\"].append(data_type)\n",
    "            if data_type in task:\n",
    "                # display_sample(sample)\n",
    "                log_dict[\"GT_age\"].append(gt_age)\n",
    "                log_dict[\"predicted_age\"].append(pred)\n",
    "                log_dict[\"absolute_error\"].append(ae)\n",
    "                log_dict[\"GT_age_group\"].append(\"-\")\n",
    "                log_dict[\"predicted_age_group\"].append(\"-\")\n",
    "                ae_sum += ae\n",
    "                kinship_counter += 1\n",
    "            else:\n",
    "                # display_sample(sample)\n",
    "                log_dict[\"GT_age\"].append(\"-\")\n",
    "                log_dict[\"predicted_age\"].append(\"-\")\n",
    "                log_dict[\"absolute_error\"].append(\"-\")\n",
    "                log_dict[\"GT_age_group\"].append(gt_age_group)\n",
    "                log_dict[\"predicted_age_group\"].append(pred_age_group)\n",
    "                if gt_age_group == pred_age_group:\n",
    "                    age_group_success += 1\n",
    "                age_counter += 1\n",
    "            mae = ae_sum / kinship_counter if kinship_counter > 0 else \"-\"\n",
    "            ag_acc = age_group_success / age_counter if age_counter > 0 else \"-\"\n",
    "            log_dict[\"mean_absolute_error\"].append(mae)\n",
    "            log_dict[\"age_group_classification_accuracy\"].append(ag_acc)\n",
    "            \n",
    "    print(\"# validation ----\", len(val_loader))\n",
    "    return ae_sum / kinship_counter, age_group_success / age_counter, log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, accuarcy, log_dict = my_evaluate(val_loader, model)  # 2m 10s\n",
    "print(mae, accuarcy, log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nia_valid_transforms_for_acrface = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "nia_face_age_valid_dataset = FaceAgeDataset(\n",
    "    valid_root_dir,\n",
    "    face_df,\n",
    "    nia_valid_transforms_for_acrface,\n",
    ")\n",
    "\n",
    "nia_face_age_valid_loader = DataLoader(\n",
    "    nia_face_age_valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag_acc 0.72\n",
    "my_evaluate(nia_face_age_valid_loader, model, (\"Individuals\", \"Family\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `evaluate_softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from main_ae import AgeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = \"/home/jongphago/nia_age/result_model/model_0\"\n",
    "model = AgeModel(END_AGE - START_AGE + 1, NUM_AGE_GROUPS)\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load(save_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_softmax(val_loader, model, criterion2):\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    softmax_loss_val = 0.0\n",
    "    mae = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, sample in tqdm(enumerate(val_loader)):\n",
    "            image = sample[\"image\"].cuda()\n",
    "            label = sample[\"age\"].cuda()\n",
    "            age_class = sample[\"age_class\"].cuda()\n",
    "            age_pred, age_group_pred = model(image)\n",
    "            data_type = sample[\"data_type\"][0]\n",
    "            if data_type in (\"Individuals\", \"Family\"):\n",
    "                loss = criterion2(age_pred, label) + criterion2(age_group_pred, age_class)\n",
    "            else:\n",
    "                loss = criterion2(age_group_pred, age_class)\n",
    "            loss_val += loss.data\n",
    "            m = torch.nn.Softmax(dim=1)\n",
    "            output_softmax = m(age_pred)\n",
    "            a = torch.arange(START_AGE, END_AGE + 1, dtype=torch.float32).cuda()\n",
    "            mean = (output_softmax * a).sum(1, keepdim=True).cpu().data.numpy()\n",
    "            pred = np.around(mean)\n",
    "            # print(\"-------pred\", pred.astype(int).item(), end='\\t')\n",
    "            # print(\"-------label\", label.cpu().data.numpy().item())\n",
    "            mae += np.absolute(pred - sample[\"age\"].cpu().data.numpy())\n",
    "    return loss_val / len(val_loader), mae / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = torch.nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_softmax(\n",
    "    nia_face_age_valid_loader,\n",
    "    model,\n",
    "    cross_entropy_loss,\n",
    ")  # mae 3.77"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `validate_aihub`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
